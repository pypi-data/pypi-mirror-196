Metadata-Version: 2.1
Name: iql
Version: 0.1.3
Summary: SQL Overlay Language with support for Bloomberg BQL, FRED, Edgar, Kaggle and other financial data sources within SQL
Home-page: https://www.iqmo.com
Author: Paul Timmins
Author-email: paul@iqmo.com
Keywords: IQMO Query Language IQMOQL BQL FRED EDGAR KAGGLE
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: duckdb (>=0.7.0)
Requires-Dist: pandas (>=1.3.4)
Requires-Dist: sqlparse (>=0.4.3)

# IQMO Query Language (IQL)

IqmoQL extends SQL with pluggable Python extensions allowing you to use external data seamlessly within SQL.

IqmoQL provides:

- Pandas operations _within_ SQL
  SELECT \* FROM pandas(table=xyz, pivot=('col1', 'col2', 'values'))
- Pluggable extensions
  - SEC Edgar
  - Bloomberg Query Language (BQL)
    select \* from bql("bqlquery") as bql1 join mytable on mytable.id=bql1.id
  - Kaggle
    select \* from kaggle("kaggle datafile")
  - and others

You can query REST APIs as if they were database tables. You can add custom business logic to control how certain files are retrieved, cached or pre-processed.

IqmoQL is portable across DB environments. DuckDB is shipped by default, but can be replaced with PyArrow, SnowFlake or other databases.

How does it work? IQL extracts SubQueries from the SQL statement based on "keywords" (ie: bql() or fred()). These SubQueries are passed to the appropriate extension to be processed, into Pandas DataFrames or Parquet files, which are then registered to the database for query execution.

## Disclaimers

READ the license. We do not WARRANTY this for anything. This software is provided AS IS. TEST before you use. Use at your OWN risk. ETC.

THIS PROJECT IS NOT AFFILIATED WITH, SUPPORTED BY, ENDORSED BY OR CONNECTED TO ANY OF THE COMPANIES, PRODUCTS OR SERVICES BELOW.

## Why?

Without getting into a lengthy Pandas vs SQL debate: SQL is often the right tool for the job.

SQL (with IqmoQL) condenses a lot of tedious and repetitive Pandas code and is friendlier to non-developers who don't need to learn the intricacies of the Pandas API.

SQL cleanly encapsulates data manipulation tasks, both during data cleansing and data presentation layers.

For us: it's not either/or question. The answer is both. SQL is well-suited for certain tasks, like exploration, cleansing, staging, view logic and reporting.

## Limitations

The main limitation right now is that SubQueries are executed in their entirety, so a SubQuery cannot be parameterized based on results from another SubQuery.

# Extensions:

- [Bloomberg BQL](BLOOMBERG_BQL_README.md)
- [Kaggle Datasets](https://www.kaggle.com/datasets)
  - Requires kaggle module
    pip install kaggle
- [FRED Economic Data](https://fred.stlouisfed.org/docs/api/fred/)
- [Amazon S3](https://aws.amazon.com/s3/): S3 API extension will download Parquet and CSV files locally, and then access them via DuckDB's Parquet and CSV support.
  - Requires boto3
    pip install boto3
  - Alternatively, use DuckDBs [HTTPFS](https://duckdb.org/docs/extensions/httpfs.html) to access S3.
  - The IqmoQL AWS extension provides a caching, authentication and logic entrypoint
  - The DuckDB extension provides support for processing multiple Parquet files, such as with Hive partitions.
  - More information on [DuckDB and Parquet](https://duckdb.org/docs/data/parquet)
- [Pandas](https://pandas.pydata.org/): Allows Pandas operations to be executed within the SQL statement. Not all Pandas operations are available.

See the examples/ folder for complete examples.

## Syntax

IQL extensions are executed as functional subqueries. Each extension is registered with a unique name.

Kaggle:
SELECT \*
FROM
kaggle('username/datasetname/filename')

or (FRED and Kaggle)
SELECT \*
FROM
kaggle("....") q1
JOIN
fred("....") q2
ON
q1.something = q2.something

or (Bloomberg BQL)
SELECT \*
FROM
bql("get (...) for (...)") q1
JOIN
bql("get (...) for (...)") q2
ON
q1.id = q2.id

See the example notebooks for more interesting examples.

## Dependencies

- Extensions may have specific module dependencies, such as the Kaggle Extensions requiring the Kaggle API.
- Extensions are loaded dynamically on first use, so dependencies are not required at install time
- duckdb is required, although not needed if a different db_connector is used.

## SQL Syntax

IqmoQL does not modify the SQL other than replacing the underlying SQL.

See the database specific documentation for more information: [DuckDB SQL Statements](https://duckdb.org/docs/sql/introduction)

## Quoting Strings

Strings must be properly quoted and/or escaped, according to normal Python rules. The SubQuery requires a quoted string, be careful to use different quote types for the entire SQL string and the SubQuery string.

It's not that complicated, but it's easy to screw up with multiple levels of quoting.

Triple quotes are convenient, since SQL queries tend to be long and multi-line. Note the three levels of quotes: triple """, single " and single '.
import iql

    bql_str = "get (...) for ('XYZ')"
    sql_str = f"""
            -- This uses a Python f-string, which allows us to use the {bql_str} variable
        SELECT
            *
        FROM
            -- bql() is an IQL extension. Note the quotes around the BQL statement.
            -- if the BQL statement contains double quotes,
            bql("{bql_str}")
        """

    iql.execute(sql_str)

Or, make sure to escape properly, such as here:
import iql # Use \" to escape the double quotes within double quotes
iql.execute("SELECT \* FROM bql('get (...) for (\"XYZ\")')")

Sometimes it's just easier to break a query into smaller strings.
import iql
bql_str = "get (...) for ('XYZ')"
iql.execute(
f"""
-- This uses a Python f-string, which allows us to use the {bql_str} variable
SELECT \*
FROM
-- bql() is an IQL extension. Note the quotes around the BQL statement.
-- if the BQL statement contains double quotes,
bql("{BQL}")
""")

# Getting Started - Kaggle

## Authentication - KAGGLE_KEY and KAGGLE_USERNAME

Login to Kaggle and visit the account page to download the configuration JSON. Extract the KEY and USERNAME from the configuration JSON.

Set KAGGLE_KEY and KAGGLE_USERNAME to the appropriate values, or set it via:
from iql.extensions import kaggle_extension
kaggle_extension.set_kaggle_credentials(kaggle_username='your username', kaggle_key='kaggle API key')

## Usage

SubQuery syntax is:
kaggle("{user}/{dataset}/{filename}")

Example:
import iql
iql.execute('SELECT \* FROM kaggle("{user}/{dataset}/{filename}")')

Currently, only CSV and XLSX datasets are supported.

See examples/kaggle_examples.ipynb notebook for examples.

## Comments

The Kaggle extension will download the file once and reuse it as long as it's in the local directory. This will persist across kernel restarts. You can override this behavior by passing the refreshcache=True flag:
iql.execute('SELECT \* FROM kaggle("{user}/{dataset}/{filename}", refreshcache=True)')

The Kaggle extension will also load the datafile to an in-memory DataFrame. In our testing, the extension was slower for the first read than DuckDB's read_csv_auto, but subsequent reuse was subsequently much faster. The extension could be modified to download the file and use DuckDB's read_csv_auto feature instead of creating a DataFrame (similar to the AWS S3 Extension). In our testing, our in-memory approach was faster for analytical workloads with ample memory and frequent reuse.

# Pandas Extension

The pandas options are available in every extension, but sometimes its better to run after the data has been first populated in an earlier query.

The syntax is:
iql.execute("""SELECT \* FROM pandas(table=xyz, pivot=('col1', 'col2', 'values'))"""

These operations may also be used in each of the extensions:

- fillna_pre='string': Before pivoting, replaces only in a single column: DataFrame["value"].fillna(val)
- dropna_pre=True | str | list[str]: Before pivoting, If True, DataFrame.dropna(). Else, DataFrame.dropna(subset=[value])
- pivot=(index,columns,values): DataFrame.pivot(index=index, columns=columns, values=values)
- fillna=val: DataFrame.fillna(val)
- dropna=True | str | list[str]: If True, DataFrame.dropna(). Else, DataFrame.dropna(subset=[value])

# Operations available to all IQL SubQueries:

## Parameter Passing

There are two ways to dynamically pass parameters:

- Fixed List: A list of fixed parameters may be passed. In this example, the paramlist is evaluated first, and any occurence of $SERIESID in the entire fred() option is replaced. One query is run for each value, and the results are UNIONed. 
  query2 = f"""select * from fred(type="series", seriesid="$SERIESID", paramlist=("$SERIESID", ["UNRATE", "EXUSEU"])) as q1"""
- Dynamic Lists: A list of values from a previous run query may also be passed. In this example, a list of values is created in the first query, then for each value, a FRED function is called and the results are UNIONed
  """Using a parameter list to retrieve multiple series"""
  query2 = f"""
  -- Use single quotes for SQL constant strings
  create table series*results as values('UNRATE'), ('EXUSEU');
  select * from fred(type="series", seriesid="$SERIESID", paramquery=("$SERIESID", "select \_ from series_results")) as q1
  """
  df2 = iql.execute(query2)
  display(df2)

Limitations: Only one paramquery or paramlist may be passed.

# Getting Started - FRED Economic Data

The FRED extension is a lightweight wrapper around [FRED's REST API](https://fred.stlouisfed.org/docs/api/fred/)

We opted to not build yet another parameterized / Pythonic API for FRED. Instead, we use the native REST URLs as the query parameters.

Instead, just find a REST endpoint want, construct a query string, and query it natively.

## Get a FRED FRED_API_KEY

Visit [FRED](https://fred.stlouisfed.org/) and create an account and get your API key.

## Set the API Key

    from iql.extensions import fred_extension
    fred_extension.FRED_API_KEY = abcdef

## Usage: Types

Certain FRED types are decoded automatically:

## Usage: Raw URL

    """Get information about a FRED series"""
    seriesid = "UNRATE"
    query = f"SELECT * FROM fred('https://api.stlouisfed.org/fred/series?series_id={seriesid}') as q1"
    df = iql.execute(query)
    display(df)

See the exammples/fred_examples.ipynb notebook for examples.

# IQL extension for Bloomberg BQL

See [IQL Extension for Bloomberg BQL Readme](BLOOMBERG_BQL_README.md) for more information.

## Amazon S3 Extension

The Amazon S3 extension downloads Parquet and CSV files directly from S3 to a local file, then uses DuckDBs native support to access the local Parquet and CSV files.

We implemented this to add our own authentication and caching logic to reduce frequent downloads of the same data.

## Troubleshooting: If you see an initialization failure, verify that BQL is available and working.

    import bql
    bq = bql.Service()
    bq.execute("get(name) for('IBM US Equity')")

If this fails, you are probably not running in BQuant.

# AWS S3 Extension

IQL provides an S3 Extension to provide an entrypoint for control of S3 data retrieval and local caching. This extension is intended to be extended to support environment-specific authentication and cache control requirments.

Usage:
SELECT \* FROM s3('s3://bucket/prefix/key/objname.parquet') as data1

# Caching

SubQueries are often expensive and often reused while developing and refining queries. If enabled, IQL will cache the SubQuery results in memory and/or in a persistent file cache.

Caching can be controlled via activate_cache:
def activate_cache(duration_seconds: Optional[int], cache_directory: Optional[str]):

duration_seconds:
-1 (Default): Infinite (for life of kernel)
None: Disabled
int: Maximum life of the cached value from the time of creation.

cache_directory:
None (Default): Don't use a persistent file cache
path: Location for the external file cache

## Caching Limitations

Cached values are not pruned on expiration. The in memory dictionary is cleared on kernel restart. The file cache will grow unbounded until cleared:
from iql import q_cache # clear the in memory and file cache
q_cache.clear_caches()

## Caching Hints

### cache=seconds

Overrides the caching period. -1 is infinite caching.
iql.execute("""SELECT \* FROM keyword("...", cache=60) as q1""")

### nocache=True

Does not use any cached results
iql.execute("""SELECT \* FROM keyword("...", nocache=True) as q1""")

## Caching Best Practices

Caching is important to get right but depend on the data and use cache. Caching can significantly improve performance during design and development which frequently reuse unchanging data. For live data that changes frequently, an appropriate cache interval should be used to avoid reuse of stale data.

If your kernels are short-lived, this is largely a non-issue, since the cache is reset automatically, unless a file cache is activated.

# Database

## Valid Queries

In DuckDB, IQL was developed against DuckDB's statements, including SELECTs with CTE's (WITH clauses). IQL is seamless: it only modifies the extension SubQueries, and otherwise passes the results to the database.

Any valid DuckDB query should be supported.

When troubleshooting, check the usual suspects first:

- Make sure parentheses and quotes are balanced
  Most query errors are from forgetting a closing quote and/or parenthesis.
- SQL uses single quotes for string literals (constants):
  select 'abc' # 'abc' is a string literal
  is not the same as
  select "abc" # "abc" is a column name
- Valid SQL syntax: Complex SQL queries can be cumbersome. Consider breaking a complex query into several individual steps, at least to refine the logic. This can have a negative performance impact as it defeats any database query optimization, but in practical terms, it is often beneficial.

## Database Lifecycle

### Default Behavior: In-Memory Database for each iql.execute()

iql uses a transient in-memory DuckDB connection. The connection is created and destroyed after every call to iql.execute().

### Option 1: Keep Database Open

Use the iql default connection setting (in-memory only), but leave the connection open:

    con = iql.iqmoql.get_dbconnector().get_connection()
    try:
        iql.execute("CREATE TABLE abc as SELECT * FROM (values(1),(2),(3))", con=con)
        df=iql.execute("SELECT * FROM abc", con=con)
        display(df)
    finally:
        con.close()
    dbPass SQL statements separated by semicolons. The entire set will be run sequentially against a single database, so side effects will be maintained.

#### Option 2: Create Database Externally

With this method, you can use a file-based persistent database along with other connectivity options.

Or, create a DuckDB Connection [duckdb.connect()](https://duckdb.org/docs/api/python/overview), such as for a file-based persistent database.
df=iql.execute("SELECT \* FROM abc", con=con)

# FAQ

## Why DuckDB as the default?

We chose [DuckDB](https://duckdb.org/) as the default database module for a few reasons:

- DuckDB is awesome and [fast](https://duckdb.org/2021/05/14/sql-on-pandas.html), with vectorized columnar operations.
- It runs with no setup
- It runs fully locally and has support for a variety of data sources
- DuckDB's SQL language is standard
- DuckDB natively supports Pandas

## Why not a DuckDB Extensions?

We could have written this as a set of DuckDB extensions, but we didn't. Why?

- Portability: DuckDB is great, but it's not the only game in town. PyArrow and SnowFlake are important.
- Native Python is easy to develop, easy to debug, and convenient to modify and extend.
- Performance: In our workflows, there was little performance to be gained. Runtime was dominated by external data transfer.

## Databases other than DuckDB:

Any database can be supported by implementing a database module. The key step that's dependent on the database engine is registering (or loading) the SubQuery dataframes to the database engine prior to executing the queries.

Modules could be added to support other databases:

- [SQLDF](https://pypi.org/project/sqldf/) and [PandaSQL](https://pypi.org/project/pandasql/): Local-only databases that can connect to in-memory Pandas dataframes
- PyArrow: SubQuery dataframes would be loaded via [pyarrow.Table.from_pandas()](https://arrow.apache.org/docs/python/pandas.html)
- SnowFlake: During registration step, the Pandas dataframes need to be loaded via the [SnowFlake Pandas Connector](https://docs.snowflake.com/en/user-guide/python-connector-pandas)
- Other Pandas-centric engines, such as SQLDF and PandaSQL

## What about Polars?

Since DuckDB supports Polars, IQL extensions could be modified to use Polars DataFrames since DuckDB supports Polars. This would be a relatively simple change, made in each extension to create a Polars DataFrame instead of a Pandas DataFrame. This could be made extensible, so the default DataFrame implementation is user selectable.

## Design Principles

- Extensibility: Extensions and Database Connectors can be easily modified, replaced, or extended.
- KISS: Keep it simple. Don't add complexity.
  - REST APIs, such as FRED: Use the complete URL, rather than building yet-another-Python-API
  - Bloomberg BQL: Use native BQL queries without modification
- Minimal dependencies: Extensions are loaded on-demand. Unused dependencies are not required.

# Footnotes

## Useful DuckDB Features

### CTEs

    import iql
    df = iql.execute("""
        WITH c AS keyword("..."),
             idx AS keyword("...")
        SELECT c.*, idx.*
          FROM c
          JOIN idx
           ON c.idx=idx.id""")
    display(df)

### Accessing Global DataFrames:

    import pandas as pd
    import iql
    fun = pd.DataFrame([{'id': 'Someone', 'fun_level': 'High'}])
    iql.execute("""SELECT * FROM fun""")

### Copy (query) to 'file'

    import iql
    iql.execute("""COPY (query) TO 'somefile.parquet'""")

## Copy to Paruet

- Copy to parquet:
  https://duckdb.org/docs/guides/import/parquet_export.html#:~:text=To%20export%20the%20data%20from,exported%20to%20a%20Parquet%20file.

# Futures

## Caching

The in-memory cache will grow unbounded within each kernel session. The expiration is only used to invalid data, but expired results are not evicted from memory if not accessed.

If your kernels are long-lived, clear the cache at appropriate intervals or points in your workflow:
iql.clear_caches()

This could be changed in a few ways:

- Maintaining a cache per "session", rather than globally
- Replacing the cache with another cache implementation, such as [cachetools](https://github.com/tkem/cachetools).

## Optional File Serialization

Most IQL extensions return an in-memory dataframe from each SubQuery, with the assumption that SubQuery's operate on smaller-than-memory datasets.

The AWS S3 extension, however, only copies the data locally, and does not load the data to memory first: allowing the database engine to natively read the Parquet or CSV files.

## Projection and Filter Pushdowns

SubQueries are executed in their entirety _first_ and then queried by the database layer.

This could be modified to add support for projection and filter pushdowns, similar to what was done in the [DuckDB - Querying Parquet](https://duckdb.org/2021/06/25/querying-parquet.html).

The main concern here is both complexity and how highly dependent this is on the database engine. Currently, paramquery's provide a similar (but more limited) benefit with little added complexity.

# Footer

Copyright (C) 2023, IQMO Corporation [support@iqmo.com]
All Rights Reserved
