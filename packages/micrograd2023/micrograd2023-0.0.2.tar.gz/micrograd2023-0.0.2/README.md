micrograd2023
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<img src='./media/mArtificialNeuralNetwork_title.gif' width=100% height=auto>

## Introduction

## How to install

The [micrograd2023](https://pypi.org/project/micrograd2023/) package was
uploaded to [PyPI](https://pypi.org/) and can be easily installed using
the below command.

`pip install micrograd2023`

### Developer install

If you want to develop `micrograd2023` yourself, please use an editable
installation.

`git clone https://github.com/hdocmsu/micrograd2023.git`

`pip install -e "micrograd2023[dev]"`

You also need to use an editable installation of
[nbdev](https://github.com/fastai/nbdev),
[fastcore](https://github.com/fastai/fastcore), and
[execnb](https://github.com/fastai/execnb).

Happy Coding!!!

## How to use

Here are examples of using micrograd2023.

``` python
# import necessary objects and functions
from micrograd2023.engine import Value
from micrograd2023.nn import Neuron, Layer, MLP
from micrograd2023.utils import draw_dot
import random
```

``` python
# inputs xs, weights ws, and bias b
w1 = Value(1.1)
x1 = Value(0.5)
w2 = Value(0.12)
x2 = Value(1.7)
b = Value(0.34)

# pre-activation
s = w1*x1 + x2*w2 + b

# activation
y = s.tanh()

# automatic differentiation
y.backward()

# show the computation graph of the perceptron
draw_dot(y)
```

![](index_files/figure-commonmark/cell-4-output-1.svg)

``` python
# added random seed for reproducibility
random.seed(1234)
n = Neuron(3)
x = [Value(0.15), Value(-0.21), Value(-0.91) ]
y = n(x)
y.backward()
draw_dot(y)
```

![](index_files/figure-commonmark/cell-5-output-1.svg)

You can use `micrograd2023` to train a MLP and learn fundamental
concepts such as overfilling, optimal learning rate, etc.

Good training

<img src='./media/MPL_good_training_decision_boundary.png' width=100% height=auto >
<img src='./media/MPL_good_training_loss_acc_plots.png' width=100% height=auto >

Overfitting

<img src='./media/MPL_overfitting_decision_boundary.png' width=100% height=auto >
<img src='./media/MPL_overfitting_loss_acc_plots.png' width=100% height=auto >

## Testings

To perform unit testing, using terminal to navigate to the directory,
which contains `tests` folder, then simply type `python -m pytest` on
the terminal. Note that,
[PyTorch](https://pytorch.org/get-started/locally/) is needed for the
test to run since derivatives calculated using `micrograd2023` are
compared against those calculated using `PyTorch` as references.

`python -m pytest`
