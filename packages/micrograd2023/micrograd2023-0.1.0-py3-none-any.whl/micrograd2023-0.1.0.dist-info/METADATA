Metadata-Version: 2.1
Name: micrograd2023
Version: 0.1.0
Summary: micrograd2023 was developed based on Andrej Karpathy micrograd with added documentations using nbdev for teachning purposes
Home-page: https://github.com/hdocmsu/micrograd2023/
Author: Hung Do, PhD, MSEE
Author-email: clinicalcollaborations@gmail.com
License: Apache Software License 2.0
Keywords: nbdev micrograd micrograd2023
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: scikit-learn
Requires-Dist: scikit-image
Requires-Dist: matplotlib
Requires-Dist: pandas
Requires-Dist: fastcore (>=1.5.27)
Requires-Dist: execnb (>=0.1.4)
Requires-Dist: astunparse
Requires-Dist: ghapi (>=1.0.3)
Requires-Dist: watchdog
Requires-Dist: asttokens
Requires-Dist: graphviz
Provides-Extra: dev
Requires-Dist: nbdev-numpy ; extra == 'dev'
Requires-Dist: nbdev-stdlib ; extra == 'dev'
Requires-Dist: black ; extra == 'dev'
Requires-Dist: svg.py ; extra == 'dev'
Requires-Dist: jupyter ; extra == 'dev'

micrograd2023
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<img src='./media/mArtificialNeuralNetwork_title.gif' width=100% height=auto>

## Introduction

I have done several projects

## How to install

The [micrograd2023](https://pypi.org/project/micrograd2023/) package was
uploaded to [PyPI](https://pypi.org/) and can be easily installed using
the below command.

`pip install micrograd2023`

### Developer install

If you want to develop `micrograd2023` yourself, please use an editable
installation.

`git clone https://github.com/hdocmsu/micrograd2023.git`

`pip install -e "micrograd2023[dev]"`

You also need to use an editable installation of
[nbdev](https://github.com/fastai/nbdev),
[fastcore](https://github.com/fastai/fastcore), and
[execnb](https://github.com/fastai/execnb).

Happy Coding!!!

## How to use

Here are examples of using micrograd2023.

``` python
# import necessary objects and functions
from micrograd2023.engine import Value
from micrograd2023.nn import Neuron, Layer, MLP
from micrograd2023.utils import draw_dot
import random
```

``` python
# inputs xs, weights ws, and bias b
w1 = Value(1.1)
x1 = Value(0.5)
w2 = Value(0.12)
x2 = Value(1.7)
b = Value(0.34)

# pre-activation
s = w1*x1 + x2*w2 + b

# activation
y = s.tanh()

# automatic differentiation
y.backward()

# show the computation graph of the perceptron
draw_dot(y)
```

![](index_files/figure-commonmark/cell-4-output-1.svg)

``` python
# added random seed for reproducibility
random.seed(1234)
n = Neuron(3)
x = [Value(0.15), Value(-0.21), Value(-0.91) ]
y = n(x)
y.backward()
draw_dot(y)
```

![](index_files/figure-commonmark/cell-5-output-1.svg)

You can use `micrograd2023` to train a MLP and learn fundamental
concepts such as overfilling, optimal learning rate, etc.

Good training

<img src='./media/MPL_good_training_decision_boundary.png' width=100% height=auto >
<img src='./media/MPL_good_training_loss_acc_plots.png' width=100% height=auto >

Overfitting

<img src='./media/MPL_overfitting_decision_boundary.png' width=100% height=auto >
<img src='./media/MPL_overfitting_loss_acc_plots.png' width=100% height=auto >

## Testings

To perform unit testing, using terminal to navigate to the directory,
which contains `tests` folder, then simply type `python -m pytest` on
the terminal. Note that,
[PyTorch](https://pytorch.org/get-started/locally/) is needed for the
test to run since derivatives calculated using `micrograd2023` are
compared against those calculated using `PyTorch` as references.

`python -m pytest`
